{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# collection of scripts from Allstate and beyond, just adding more information\n",
    "# got 0.585 from gtx 1060 6g in 2h great to see speed of Titan X!\n",
    "import os\n",
    "import sys\n",
    "import operator\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy import sparse\n",
    "from sklearn import model_selection, preprocessing, ensemble\n",
    "from sklearn.metrics import log_loss\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from scipy.stats import zscore\n",
    "import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.cross_validation import KFold\n",
    "#from sklearn.model_selection import KFold\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.layers.advanced_activations import PReLU\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "\n",
    "from keras.utils.np_utils import to_categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "import pandas as pd\n",
    "\n",
    "test_df1 = pd.DataFrame()\n",
    "test_df1 = test_df1.from_csv('/Users/ethanweber/documents/nycdsa-proj-03/data/test-v13.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_df1 = pd.DataFrame()\n",
    "train_df1 = train_df1.from_csv('/Users/ethanweber/documents/nycdsa-proj-03/data/train-v13.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#test_df = \n",
    "#list(pd.concat([test_df1.iloc[:, :14], test_df1.iloc[:, 23:]], axis = 1))\n",
    "#train_df = pd.concat([train_df1.iloc[:, :14], train_df1.iloc[:, 23:]], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "features_to_use  = [\"bathrooms\", \"bedrooms\", \"latitude\", \"longitude\", \"Predicted_feats\", \"price\", \"sum_vec\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# convert the created column to datetime object so as to extract more features \n",
    "train_df[\"created\"] = pd.to_datetime(train_df[\"created\"])\n",
    "test_df[\"created\"] = pd.to_datetime(test_df[\"created\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Let us extract some features like year, month, day, hour from date columns #\n",
    "train_df[\"created_year\"] = train_df[\"created\"].dt.year\n",
    "test_df[\"created_year\"] = test_df[\"created\"].dt.year\n",
    "train_df[\"created_month\"] = train_df[\"created\"].dt.month\n",
    "test_df[\"created_month\"] = test_df[\"created\"].dt.month\n",
    "train_df[\"created_day\"] = train_df[\"created\"].dt.day\n",
    "test_df[\"created_day\"] = test_df[\"created\"].dt.day\n",
    "train_df[\"created_hour\"] = train_df[\"created\"].dt.hour\n",
    "test_df[\"created_hour\"] = test_df[\"created\"].dt.hour"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "features_to_use.extend([\"photoCount\", \"featureCount\", \"created_year\", \"created_month\", \"created_day\", \"listing_id\", \"created_hour\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# adding all these new features to use list #\n",
    "features_to_use.extend([\"created_year\", \"created_month\", \"created_day\", \"listing_id\", \"created_hour\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "categorical = [\"manager_id\", \"building_id\", \"street_address\"]\n",
    "for f in categorical:\n",
    "        if train_df[f].dtype=='object':\n",
    "            #print(f)\n",
    "            lbl = preprocessing.LabelEncoder()\n",
    "            lbl.fit(list(train_df[f].values) + list(test_df[f].values))\n",
    "            train_df[f] = lbl.transform(list(train_df[f].values))\n",
    "            test_df[f] = lbl.transform(list(test_df[f].values))\n",
    "            features_to_use.append(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#print(train_df[\"features\"].head())\n",
    "#tfidf = TfidfVectorizer(stop_words='english', max_features=15)\n",
    "#tr_sparse = tfidf.fit_transform(train_df[\"features\"])\n",
    "#te_sparse = tfidf.transform(test_df[\"features\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_X = train_df[features_to_use]\n",
    "test_X = test_df[features_to_use]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(49352, 19)\n",
      "(74659, 19)\n"
     ]
    }
   ],
   "source": [
    "train_X = train_X.__array__()\n",
    "test_X = test_X.__array__()\n",
    "print(train_X.shape)\n",
    "print(test_X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Scale train_X and test_X together\n",
    "traintest = np.vstack((train_X, test_X))\n",
    "\n",
    "traintest = preprocessing.StandardScaler().fit_transform(traintest)\n",
    "\n",
    "train_X = traintest[range(train_X.shape[0])]\n",
    "test_X = traintest[range(train_X.shape[0], traintest.shape[0])]\n",
    "\n",
    "## neural net\n",
    "def nn_model():\n",
    "    model = Sequential()\n",
    "    \n",
    "    model.add(Dense(500, input_dim = train_X.shape[1], init = 'he_normal', activation='sigmoid'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dropout(0.35))\n",
    "    model.add(PReLU())\n",
    "    \n",
    "    model.add(Dense(50, init = 'he_normal', activation='sigmoid'))\n",
    "    model.add(BatchNormalization())    \n",
    "    model.add(Dropout(0.35))\n",
    "    model.add(PReLU())\n",
    "\t\n",
    "    model.add(Dense(3, init = 'he_normal', activation='softmax'))\n",
    "    model.compile(loss = 'categorical_crossentropy', optimizer = 'adam')#, metrics=['accuracy'])\n",
    "    return(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "target_num_map = {'high':0, 'medium':1, 'low':2}\n",
    "train_y = np.array(train_df['interest_level'].apply(lambda x: target_num_map[x]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_y = to_categorical(train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "do_all = True\n",
    "## cv-folds\n",
    "nfolds = 10\n",
    "if do_all:\n",
    "\tif nfolds>1:\n",
    "\t\tfolds = KFold(int(len(train_y)), n_folds = nfolds, shuffle = True, random_state = 111)\n",
    "\tpred_oob = np.zeros((len(train_y), 3))\n",
    "\ttestset = test_X\n",
    "else:\n",
    "\tfolds = KFold(int(len(train_y)*0.8), n_folds = nfolds, shuffle = True, random_state = 111)\n",
    "\tpred_oob = np.zeros((int(len(train_y)*0.8), 3))\n",
    "\ttestset = train_X[range(int(len(train_y)*0.8), len(train_y))]\n",
    "\tytestset = train_y[int(len(train_y)*0.8):(len(train_y))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "0.620955985972\n",
      "1:09:00.258569\n",
      "1\n",
      "0.613068251096\n",
      "2:17:07.391126\n",
      "2\n",
      "0.61043825582\n",
      "3:22:50.722136\n",
      "3\n",
      "0.610013324025\n",
      "4:27:33.182186\n",
      "4\n",
      "0.609203620926\n",
      "5:37:19.868958\n",
      "('Fold ', 1, '- logloss:', 0.60920362092560409)\n",
      "0\n",
      "0.608513151853\n",
      "6:45:39.278567\n",
      "1\n",
      "0.604839371079\n",
      "7:55:29.235197\n",
      "2\n",
      "0.603437947356\n",
      "9:03:44.137994\n",
      "3\n",
      "0.60335891413\n",
      "10:08:49.886588\n",
      "4\n"
     ]
    }
   ],
   "source": [
    "## train models\n",
    "nbags = 5\n",
    "\n",
    "from time import time\n",
    "import datetime\n",
    "\n",
    "pred_test = np.zeros((testset.shape[0], 3))\n",
    "begintime = time()\n",
    "count = 0\n",
    "filepath=\"weights.best.hdf5\"\n",
    "if nfolds>1:\n",
    "\tfor (inTr, inTe) in folds:\n",
    "\t    count += 1\n",
    "\t    \n",
    "\t    xtr = train_X[inTr]\n",
    "\t    ytr = train_y[inTr]\n",
    "\t    xte = train_X[inTe]\n",
    "\t    yte = train_y[inTe]\n",
    "\t    pred = np.zeros((xte.shape[0], 3))\n",
    "\t    for j in range(nbags):\n",
    "\t        print(j)\n",
    "\t        model = nn_model()\n",
    "\t        early_stop = EarlyStopping(monitor='val_loss', patience=75, verbose=0)\n",
    "\t        checkpoint = ModelCheckpoint(filepath, monitor='val_loss', verbose=0, save_best_only=True)\n",
    "\t        \n",
    "\t        model.fit(xtr, ytr, nb_epoch = 1200, batch_size=1000, verbose = 0, validation_data=[xte, yte])\n",
    "\n",
    "\t        pred += model.predict_proba(x=xte, verbose=0)\n",
    "\t        \n",
    "\t        pred_test += model.predict_proba(x=testset, verbose=0)\n",
    "\t        \n",
    "\t        print(log_loss(yte,pred/(j+1)))\n",
    "\t        if  not do_all:\n",
    "\t        \tprint(log_loss(ytestset,pred_test/(j+1+count*nbags)))\n",
    "\t        print(str(datetime.timedelta(seconds=time()-begintime)))\n",
    "\t    pred /= nbags\n",
    "\t    pred_oob[inTe] = pred\n",
    "\t    score = log_loss(yte,pred)\n",
    "\t    print('Fold ', count, '- logloss:', score)\n",
    "\t    if not do_all:\n",
    "\t    \tprint(log_loss(ytestset, pred_test/(nbags * count)))\n",
    "else:\n",
    "    for j in range(nbags):\n",
    "        print(j)\n",
    "        model = nn_model()\n",
    "        model.fit(train_X, train_y, nb_epoch = 1200, batch_size=1000, verbose = 0)\n",
    "        pred_test += model.predict_proba(x=testset, verbose=0)\n",
    "        print(str(datetime.timedelta(seconds=time()-begintime)))\n",
    "\n",
    "if nfolds>1:\n",
    "\tif do_all:\n",
    "\t\tprint('Total - logloss:', log_loss(train_y, pred_oob))\n",
    "\telse:\n",
    "\t\tprint('Total - logloss:', log_loss(train_y[0:int(len(train_y)*0.8)], pred_oob))\n",
    "\n",
    "\n",
    "if do_all:\n",
    "\t## train predictions\n",
    "\tif nfolds>1:\n",
    "\t\tout_df = pd.DataFrame(pred_oob)\n",
    "\t\tout_df.columns = [\"high\", \"medium\", \"low\"]\n",
    "\t\tout_df[\"listing_id\"] = train_df.listing_id.values\n",
    "\t\tout_df.to_csv(\"keras_starter_train.csv\", index=False)\n",
    "\n",
    "\t## test predictions\n",
    "\tpred_test /= (nfolds*nbags)\n",
    "\tout_df = pd.DataFrame(pred_test)\n",
    "\tout_df.columns = [\"high\", \"medium\", \"low\"]\n",
    "\tout_df[\"listing_id\"] = test_df.listing_id.values\n",
    "\tout_df.to_csv(\"keras_starter_test_full.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
