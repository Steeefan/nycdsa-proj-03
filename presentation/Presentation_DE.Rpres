Machine_Learning_Presentation
========================================================
author: 
date: 
autosize: true

First Slide
========================================================

Dummy Feature Columns

- Picked 20 most common
- Gave either a 1 or a 0 depending on if that feature was present
- Allowed us to seperate features to see which are most predictive


========================================================

Percentile Columns

- Used Dplyr to group by training data by manager ID and building ID
- Counted number of listings per manager ID and building ID 
- Sorted by these counts and created variables for Top 1%, Top 5%, Top 10%, and Top 25% for building ID and manager ID

========================================================

Tuning XGBoost 

- Decided against grid search, 
- Created google doc with parameters used and the resulting log loss
- In general, found we were overfitting, tuned parameters to account for this

![img/bias_variance_tradeoff.jpeg](img/bias_variance_tradeoff.jpeg)

========================================================

Tuning XGBoost - Eta

- Eta = learning Rate
- Get weights after each boost step and shrinks them
- Smaller values prevent overfitting
- We got our best results with an 0.01 eta


========================================================

Tuning XGBoost - Gamma

- Gamma = Controls regularization
- Loss reduction required to make a partition on the leaf node
- Larger values prevent overfitting
- We got our best results with an 0.175 gamma
- Tradeoff between bias and variance 

========================================================

Tuning XGBoost - Max Depth

- Max Depth = number of levels trees are allowed to grow
- Trees that are allowed to grow to deep are generally overfitted 
- Keeping this number low can help prevent overfitting
- We found 7 was the best value for us

========================================================

TuningXGBoost - Column Sample By Tree

- Number of columns randomly sampled by each tree
- Not using 100% of the columns helps prevent overfitting
- We found that 80% was the most effective value

========================================================

TuningXGBoost - Subsample

- Number of rows randomly sampled
- Not using 100% of the rows helps to prevent overfitting
- We found that 80% gave us the best validation and test accuracy

========================================================

