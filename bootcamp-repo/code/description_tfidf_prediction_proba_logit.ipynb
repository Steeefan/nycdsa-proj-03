{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#cleaning description text in training set\n",
    "import pandas as pd\n",
    "full_df = pd.read_csv('train-v7.csv')\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.metrics.pairwise import euclidean_distances\n",
    "\n",
    "import nltk\n",
    "from nltk.tag import pos_tag\n",
    "from nltk import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "stop = stopwords.words('english')\n",
    "def cleaning_text(sentence):\n",
    "    sentence = str(sentence)\n",
    "    sentence=sentence.lower()\n",
    "    sentence=re.sub('[^\\w\\s]',' ', sentence) #removes punctuations\n",
    "    sentence=re.sub('_',' ', sentence) #removes punctuations\n",
    "    sentence=re.sub('\\d+',' ', sentence) #removes digits\n",
    "    cleaned=' '.join([w for w in sentence.split() if not w in stop]) # removes english stopwords\n",
    "    cleaned=' '.join([w for w , pos in pos_tag(cleaned.split()) if (pos == 'NN' or pos=='JJ' or pos=='JJR' or pos=='JJS' )])\n",
    "    \n",
    "#selecting only nouns and adjectives\n",
    "    cleaned=' '.join([w for w in cleaned.split() if not len(w)<=2 ]) #removes single lettered words and digits\n",
    "    cleaned=cleaned.strip()\n",
    "    return cleaned\n",
    "\t  \n",
    "full_df['cleaned'] = full_df['description'].apply(lambda x: cleaning_text(x))\n",
    "trimDF_train = full_df[['aptID','cleaned','interest_level']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "text_train = trimDF['cleaned']\n",
    "import numpy as np\n",
    "text_train = text_train.replace(np.nan, '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from time import time\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "from sklearn.datasets import fetch_20newsgroups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "n_features = 1000\n",
    "n_topics = 10\n",
    "n_top_words = 20\n",
    "\n",
    "\n",
    "tf_vectorizer = CountVectorizer(max_df=0.95, min_df=2, max_features=n_features,\n",
    "                                stop_words='english')\n",
    "tf = tf_vectorizer.fit_transform(text_train)\n",
    "TF = TfidfTransformer()\n",
    "tf_idf = TF.fit_transform(tf)\n",
    "tf_idf = tf_idf.toarray() #convert sparse matrix to array\n",
    "#tf_idf.toarray()[:100, :100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tf_idf = pd.DataFrame(tf_idf)\n",
    "trimDF_train = pd.concat([trimDF_train.drop(['cleaned','Unnamed: 0'], axis=1), tf_idf.add_prefix('tf_idf')], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#logistic regression model predicting based on df_idf\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import pickle\n",
    "#nrow = trimDF_tfidf.shape[0]\n",
    "\n",
    "train, test = train_test_split(trimDF_tfidf, train_size=0.5, random_state=1)\n",
    "Train = pd.DataFrame(train, columns=train.columns)\n",
    "Test = pd.DataFrame(test, columns=test.columns)\n",
    "\n",
    "x = Train[Train.columns.values[2:1002]]\n",
    "y = Train['interest_level']\n",
    "x2 = Test[Test.columns.values[2:1002]]\n",
    "y2 = Train['interest_level']\n",
    "\n",
    "x_total = trimDF_tfidf[trimDF_tfidf.columns.values[2:1002]]\n",
    "y_total = trimDF_tfidf['interest_level']\n",
    "model_total = LogisticRegression()\n",
    "model_total.fit(x_total,y_total)\n",
    "#use first half data train model and predict the second half\n",
    "model = LogisticRegression()\n",
    "model.fit(x, y)\n",
    "model.predict(x2)\n",
    "new_col_Test = model.predict_proba(x2)\n",
    "# save the model to disk\n",
    "filename = 'description_prediction_model_log.sav'\n",
    "pickle.dump(model, open(filename, 'wb'))\n",
    "\n",
    "model_2 = LogisticRegression()\n",
    "model_2.fit(x2, y2)\n",
    "model_2.predict(x)\n",
    "new_col_Train = model_2.predict_proba(x)\n",
    "# save the model to disk\n",
    "filename = 'description_prediction_model_2_log.sav'\n",
    "pickle.dump(model_2, open(filename, 'wb'))\n",
    "\n",
    "\n",
    "#type(new_col_1)\n",
    "col_Test = ['high','low','medium']\n",
    "new_col_Test = pd.DataFrame(new_col_Test, columns = col_Test)\n",
    "\n",
    "col_Train = ['high','low','medium']\n",
    "new_col_Train = pd.DataFrame(new_col_Train, columns = col_Train)\n",
    "#model.predict(x[:100])\n",
    "\n",
    "Train_tfidf = pd.concat([Train.ix[:,[0]].reset_index(drop=True),new_col_Train.ix[:,[0,2]]],axis=1)\n",
    "Test_tfidf = pd.concat([Test.ix[:,[0]].reset_index(drop=True),new_col_Test.ix[:,[0,2]]],axis=1)\n",
    "\n",
    "trimDF_train_prob = Train_tfidf.append(Test_tfidf, ignore_index=True)\n",
    "\n",
    "#fit model with whole training set\n",
    "x_total = trimDF_tfidf[trimDF_tfidf.columns.values[2:1002]]\n",
    "y_total = trimDF_tfidf['interest_level']\n",
    "model_total = LogisticRegression()\n",
    "model_total.fit(x_total,y_total)\n",
    "# save the model to disk\n",
    "filename = 'description_prediction_model_total_log.sav'\n",
    "pickle.dump(model_total, open(filename, 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "trimDF_train_prob.to_csv('description_prob_train_logit_v4.csv', index =False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#cleaning description text in test set\n",
    "import pandas as pd\n",
    "full_df = pd.read_csv('test-v7.csv')\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.metrics.pairwise import euclidean_distances\n",
    "\n",
    "import nltk\n",
    "from nltk.tag import pos_tag\n",
    "from nltk import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "stop = stopwords.words('english')\n",
    "def cleaning_text(sentence):\n",
    "    sentence = str(sentence)\n",
    "    sentence=sentence.lower()\n",
    "    sentence=re.sub('[^\\w\\s]',' ', sentence) #removes punctuations\n",
    "    sentence=re.sub('_',' ', sentence) #removes punctuations\n",
    "    sentence=re.sub('\\d+',' ', sentence) #removes digits\n",
    "    cleaned=' '.join([w for w in sentence.split() if not w in stop]) # removes english stopwords\n",
    "    cleaned=' '.join([w for w , pos in pos_tag(cleaned.split()) if (pos == 'NN' or pos=='JJ' or pos=='JJR' or pos=='JJS' )])\n",
    "    \n",
    "#selecting only nouns and adjectives\n",
    "    cleaned=' '.join([w for w in cleaned.split() if not len(w)<=2 ]) #removes single lettered words and digits\n",
    "    cleaned=cleaned.strip()\n",
    "    return cleaned\n",
    "\t  \n",
    "full_df['cleaned'] = full_df['description'].apply(lambda x: cleaning_text(x))\n",
    "trimDF_test = full_df[['aptID','cleaned']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "text = trimDF_test['cleaned']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#delete all nan in text\n",
    "import numpy as np\n",
    "text = text.replace(np.nan, '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from time import time\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "from sklearn.datasets import fetch_20newsgroups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#computing the term frequency for the selected words:\n",
    "n_features = 1000\n",
    "n_topics = 10\n",
    "n_top_words = 20\n",
    "\n",
    "\n",
    "tf_vectorizer = CountVectorizer(max_df=0.95, min_df=2, max_features=n_features,\n",
    "                                stop_words='english')\n",
    "tf = tf_vectorizer.fit_transform(text)\n",
    "TF = TfidfTransformer()\n",
    "#compute tf_idf\n",
    "tf_idf = TF.fit_transform(tf)\n",
    "tf_idf = tf_idf.toarray() #convert sparse matrix to array\n",
    "#tf_idf.toarray()[:100, :100]\n",
    "tf_idf = pd.DataFrame(tf_idf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "trimDF_test_tfidf = pd.concat([trimDF_test.drop(['cleaned','Unnamed: 0'], axis=1), tf_idf.add_prefix('tf_idf')], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "X = trimDF_tfidf[trimDF_tfidf.columns.values[1:1002]]\n",
    "# load the model from disk\n",
    "loaded_model = pickle.load(open('description_prediction_model_total_log.sav', 'rb'))\n",
    "result = loaded_model.predict_proba(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "col_test = ['high','low','medium']\n",
    "result = pd.DataFrame(result, columns = col_test)\n",
    "\n",
    "colsToDrop = ['low']\n",
    "result = result.drop(colsToDrop, axis=1)\n",
    "\n",
    "test_prob = pd.concat([trimDF_tfidf.ix[:,[0]].reset_index(drop=True),result.ix[:,[0,1]]],axis=1)\n",
    "\n",
    "test_prob.to_csv('description_prob_test_logit_v4.csv', index =False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
